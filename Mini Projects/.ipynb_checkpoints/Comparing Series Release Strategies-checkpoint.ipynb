{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Series Release Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project I try to answer the question: which release strategy produces more positive discussions; releasing all episodes at once or releasing them once every week? I try to answer this question by looking at the most distinct adjectives used in the discussions of series on Reddit.\n",
    "\n",
    "Datasets used in this mini project:\n",
    "**discussions_corrected.csv**: 50,000 Reddit comments on episode threads of popular television series between 2011 and 2020.\n",
    "\n",
    "The discussions dataset contains posts about series that release their season all at once and release an episode every week. Therefore, by splitting the data, these posts can be compared. In the posts, viewers express themselves about the series or the episodes of the series. Usually, people express themselves positively or negatively using adjectives, such as 'good' or 'bad' and 'great' or 'terrible'. These kinds of words can give an indication of whether the posts are positive or negative towards the series. Therefore, if you would look at what adjectives set the strategies apart most, this could give an indication of how the different release strategies are being received by looking at whether these adjectives are more positive or more negative. If the strategies are compared to each other, and the comparison shows that there are significantly more positive words in the highly distinctive adjectives of one release strategy, this would indicate that these positive words are used much more often to describe those series using that strategy than the other. Hence, this would indicate that that strategy should yield more positive discussions.\n",
    "\n",
    "This project was initially a school assignment for my Master's degree at Utrecht University. Hence, some of the code used in this notebook was adjusted from the code used in this lab manual: https://jveerbeek.gitlab.io/dm-manual/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import spacy \n",
    "from collections import Counter\n",
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I import the discussions dataset containing the posts about the different shows. Then, I process the posts using Spacy and make two subsets of the posts based on the two different release strategies so these strategies can be compared. Finally, I lemmatize both subsets using Spacy. I choose to lemmatize instead of stem them because this way I can filter the adjectives by using the pos tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('discussions_corrected.csv')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "texts = df.post\n",
    "texts = [text.lower() for text in texts]\n",
    "processed_texts = [text for text in nlp.pipe(texts, \n",
    "                                             disable=[\"ner\",\n",
    "                                                      \"parser\"])]\n",
    "df['processed_texts'] = processed_texts\n",
    "\n",
    "processed_lin = df[df.type == 'linear'].processed_texts\n",
    "processed_net = df[df.type == 'netflix'].processed_texts\n",
    "\n",
    "lemmatized_lin = [[token.lemma_ for token in text if not token.is_punct and token.pos_ == 'ADJ'] for text in processed_lin]\n",
    "lemmatized_net = [[token.lemma_ for token in text if not token.is_punct and token.pos_ == 'ADJ'] for text in processed_net]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Most Distinctive Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I calculate the most dinstinctive adjectives and flatten the lemmatized subsets of the discussions dataset. The results yield the log likelihood ratios which indicate how distinct a word is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinctive_words(target_corpus, reference_corpus):\n",
    "    counts_c1 = Counter(target_corpus) # don't forget to flatten your texts!\n",
    "    counts_c2 = Counter(reference_corpus)\n",
    "    vocabulary = set(list(counts_c1.keys()) + list(counts_c2.keys()))\n",
    "    freq_c1_total = sum(counts_c1.values()) \n",
    "    freq_c2_total = sum(counts_c2.values()) \n",
    "    results = []\n",
    "    for word in vocabulary:\n",
    "        freq_c1 = counts_c1[word]\n",
    "        freq_c2 = counts_c2[word]\n",
    "        freq_c1_other = freq_c1_total - freq_c1\n",
    "        freq_c2_other = freq_c2_total - freq_c2\n",
    "        llr, p_value,_,_ = chi2_contingency([[freq_c1, freq_c2], \n",
    "                      [freq_c1_other, freq_c2_other]],\n",
    "                      lambda_='log-likelihood') \n",
    "        if freq_c2 / freq_c2_other > freq_c1 / freq_c1_other:\n",
    "            llr = -llr\n",
    "        result = {'word':word, \n",
    "                    'llr':llr,\n",
    "                    'p_value': p_value}\n",
    "        results.append(result)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "results_df = distinctive_words(flatten(lemmatized_lin), flatten(lemmatized_net))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, I sort the results based on the log likelihood ratio in both ascending and descending order to show the most distinctive words for both strategies. To treat the most distinct words fairly between the strategies, I choose to select all results with either a llr >= 15 for words that are most distinctive in the first corpus or a llr <= -15 for words that are most distinctive in the second corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>llr</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>dead</td>\n",
       "      <td>95.675851</td>\n",
       "      <td>1.353211e-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>twin</td>\n",
       "      <td>84.182290</td>\n",
       "      <td>4.511944e-20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1943</th>\n",
       "      <td>next</td>\n",
       "      <td>72.757673</td>\n",
       "      <td>1.465872e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>stark</td>\n",
       "      <td>65.121098</td>\n",
       "      <td>7.043391e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>faceless</td>\n",
       "      <td>54.357545</td>\n",
       "      <td>1.671351e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4918</th>\n",
       "      <td>last</td>\n",
       "      <td>44.153454</td>\n",
       "      <td>3.036156e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3015</th>\n",
       "      <td>blue</td>\n",
       "      <td>43.063951</td>\n",
       "      <td>5.297948e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3815</th>\n",
       "      <td>valyrian</td>\n",
       "      <td>41.645536</td>\n",
       "      <td>1.094149e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>red</td>\n",
       "      <td>40.339194</td>\n",
       "      <td>2.134829e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>gus</td>\n",
       "      <td>39.740224</td>\n",
       "      <td>2.900891e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3094</th>\n",
       "      <td>ned</td>\n",
       "      <td>34.985976</td>\n",
       "      <td>3.320886e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4153</th>\n",
       "      <td>unsullied</td>\n",
       "      <td>32.140638</td>\n",
       "      <td>1.434061e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2409</th>\n",
       "      <td>whiterose</td>\n",
       "      <td>30.247277</td>\n",
       "      <td>3.803249e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2808</th>\n",
       "      <td>true</td>\n",
       "      <td>27.251331</td>\n",
       "      <td>1.786521e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4337</th>\n",
       "      <td>awesome</td>\n",
       "      <td>24.838520</td>\n",
       "      <td>6.233919e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2327</th>\n",
       "      <td>epic</td>\n",
       "      <td>23.415164</td>\n",
       "      <td>1.305454e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1893</th>\n",
       "      <td>tywin</td>\n",
       "      <td>23.091315</td>\n",
       "      <td>1.544869e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>badass</td>\n",
       "      <td>21.427955</td>\n",
       "      <td>3.673755e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>about</td>\n",
       "      <td>20.355266</td>\n",
       "      <td>6.431585e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>faced</td>\n",
       "      <td>18.970744</td>\n",
       "      <td>1.327382e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3404</th>\n",
       "      <td>mad</td>\n",
       "      <td>18.947034</td>\n",
       "      <td>1.343980e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3984</th>\n",
       "      <td>purple</td>\n",
       "      <td>18.695870</td>\n",
       "      <td>1.533142e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4231</th>\n",
       "      <td>good</td>\n",
       "      <td>18.491108</td>\n",
       "      <td>1.706988e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4157</th>\n",
       "      <td>loyal</td>\n",
       "      <td>18.064561</td>\n",
       "      <td>2.135393e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>big</td>\n",
       "      <td>16.237396</td>\n",
       "      <td>5.588012e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3281</th>\n",
       "      <td>hound</td>\n",
       "      <td>15.242250</td>\n",
       "      <td>9.456398e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>high</td>\n",
       "      <td>15.143945</td>\n",
       "      <td>9.961728e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           word        llr       p_value\n",
       "1972       dead  95.675851  1.353211e-22\n",
       "2681       twin  84.182290  4.511944e-20\n",
       "1943       next  72.757673  1.465872e-17\n",
       "5118      stark  65.121098  7.043391e-16\n",
       "2730   faceless  54.357545  1.671351e-13\n",
       "4918       last  44.153454  3.036156e-11\n",
       "3015       blue  43.063951  5.297948e-11\n",
       "3815   valyrian  41.645536  1.094149e-10\n",
       "2553        red  40.339194  2.134829e-10\n",
       "1425        gus  39.740224  2.900891e-10\n",
       "3094        ned  34.985976  3.320886e-09\n",
       "4153  unsullied  32.140638  1.434061e-08\n",
       "2409  whiterose  30.247277  3.803249e-08\n",
       "2808       true  27.251331  1.786521e-07\n",
       "4337    awesome  24.838520  6.233919e-07\n",
       "2327       epic  23.415164  1.305454e-06\n",
       "1893      tywin  23.091315  1.544869e-06\n",
       "998      badass  21.427955  3.673755e-06\n",
       "1616      about  20.355266  6.431585e-06\n",
       "1444      faced  18.970744  1.327382e-05\n",
       "3404        mad  18.947034  1.343980e-05\n",
       "3984     purple  18.695870  1.533142e-05\n",
       "4231       good  18.491108  1.706988e-05\n",
       "4157      loyal  18.064561  2.135393e-05\n",
       "106         big  16.237396  5.588012e-05\n",
       "3281      hound  15.242250  9.456398e-05\n",
       "4742       high  15.143945  9.961728e-05"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df.llr >= 15].sort_values('llr', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>llr</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3407</th>\n",
       "      <td>black</td>\n",
       "      <td>-203.430374</td>\n",
       "      <td>3.726307e-46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2717</th>\n",
       "      <td>different</td>\n",
       "      <td>-105.128149</td>\n",
       "      <td>1.144869e-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3648</th>\n",
       "      <td>real</td>\n",
       "      <td>-73.962969</td>\n",
       "      <td>7.959630e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1402</th>\n",
       "      <td>young</td>\n",
       "      <td>-69.461163</td>\n",
       "      <td>7.793407e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>nosedive</td>\n",
       "      <td>-57.311212</td>\n",
       "      <td>3.720284e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>digital</td>\n",
       "      <td>-50.699373</td>\n",
       "      <td>1.076540e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647</th>\n",
       "      <td>virtual</td>\n",
       "      <td>-47.584936</td>\n",
       "      <td>5.267168e-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4471</th>\n",
       "      <td>serial</td>\n",
       "      <td>-46.116226</td>\n",
       "      <td>1.114417e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2583</th>\n",
       "      <td>russian</td>\n",
       "      <td>-41.021682</td>\n",
       "      <td>1.505497e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3033</th>\n",
       "      <td>netflix</td>\n",
       "      <td>-34.868110</td>\n",
       "      <td>3.528125e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>913</th>\n",
       "      <td>tronte</td>\n",
       "      <td>-34.868110</td>\n",
       "      <td>3.528125e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2061</th>\n",
       "      <td>social</td>\n",
       "      <td>-33.792840</td>\n",
       "      <td>6.130407e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5361</th>\n",
       "      <td>other</td>\n",
       "      <td>-32.029533</td>\n",
       "      <td>1.518465e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3391</th>\n",
       "      <td>normal</td>\n",
       "      <td>-31.892167</td>\n",
       "      <td>1.629731e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>pregnant</td>\n",
       "      <td>-28.352488</td>\n",
       "      <td>1.011168e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>main</td>\n",
       "      <td>-25.729776</td>\n",
       "      <td>3.927209e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3999</th>\n",
       "      <td>strange</td>\n",
       "      <td>-25.343004</td>\n",
       "      <td>4.798897e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>human</td>\n",
       "      <td>-25.051495</td>\n",
       "      <td>5.581947e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>interactive</td>\n",
       "      <td>-20.091932</td>\n",
       "      <td>7.380736e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4179</th>\n",
       "      <td>apocalyptic</td>\n",
       "      <td>-20.091932</td>\n",
       "      <td>7.380736e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5086</th>\n",
       "      <td>folt</td>\n",
       "      <td>-20.091932</td>\n",
       "      <td>7.380736e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>bipolar</td>\n",
       "      <td>-19.163014</td>\n",
       "      <td>1.200164e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>gay</td>\n",
       "      <td>-18.944377</td>\n",
       "      <td>1.345854e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>461</th>\n",
       "      <td>favourite</td>\n",
       "      <td>-18.632614</td>\n",
       "      <td>1.584859e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5331</th>\n",
       "      <td>overprotective</td>\n",
       "      <td>-18.265891</td>\n",
       "      <td>1.921165e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4103</th>\n",
       "      <td>advanced</td>\n",
       "      <td>-17.626952</td>\n",
       "      <td>2.687525e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>dystopian</td>\n",
       "      <td>-17.458874</td>\n",
       "      <td>2.935906e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5230</th>\n",
       "      <td>confused</td>\n",
       "      <td>-17.000071</td>\n",
       "      <td>3.737842e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>conscious</td>\n",
       "      <td>-16.931017</td>\n",
       "      <td>3.876300e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4549</th>\n",
       "      <td>same</td>\n",
       "      <td>-15.969008</td>\n",
       "      <td>6.438799e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5172</th>\n",
       "      <td>old</td>\n",
       "      <td>-15.713446</td>\n",
       "      <td>7.369840e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                word         llr       p_value\n",
       "3407           black -203.430374  3.726307e-46\n",
       "2717       different -105.128149  1.144869e-24\n",
       "3648            real  -73.962969  7.959630e-18\n",
       "1402           young  -69.461163  7.793407e-17\n",
       "536         nosedive  -57.311212  3.720284e-14\n",
       "1247         digital  -50.699373  1.076540e-12\n",
       "647          virtual  -47.584936  5.267168e-12\n",
       "4471          serial  -46.116226  1.114417e-11\n",
       "2583         russian  -41.021682  1.505497e-10\n",
       "3033         netflix  -34.868110  3.528125e-09\n",
       "913           tronte  -34.868110  3.528125e-09\n",
       "2061          social  -33.792840  6.130407e-09\n",
       "5361           other  -32.029533  1.518465e-08\n",
       "3391          normal  -31.892167  1.629731e-08\n",
       "4175        pregnant  -28.352488  1.011168e-07\n",
       "2735            main  -25.729776  3.927209e-07\n",
       "3999         strange  -25.343004  4.798897e-07\n",
       "895            human  -25.051495  5.581947e-07\n",
       "3314     interactive  -20.091932  7.380736e-06\n",
       "4179     apocalyptic  -20.091932  7.380736e-06\n",
       "5086            folt  -20.091932  7.380736e-06\n",
       "19           bipolar  -19.163014  1.200164e-05\n",
       "488              gay  -18.944377  1.345854e-05\n",
       "461        favourite  -18.632614  1.584859e-05\n",
       "5331  overprotective  -18.265891  1.921165e-05\n",
       "4103        advanced  -17.626952  2.687525e-05\n",
       "392        dystopian  -17.458874  2.935906e-05\n",
       "5230        confused  -17.000071  3.737842e-05\n",
       "598        conscious  -16.931017  3.876300e-05\n",
       "4549            same  -15.969008  6.438799e-05\n",
       "5172             old  -15.713446  7.369840e-05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df[results_df.llr <= -15].sort_values('llr', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most distinctive adjectives of the first corpus, which is the corpus with shows that release their episodes linearly, contain the positive adjectives 'awesome', 'epic', 'good' and 'badass' while the other corpus contains none of these kinds of positive adjectives. Thus, there are four positive adjectives that are distinctively used in posts of series with the linear release strategy as opposed to zero positive adjectives for the other strategy. Moreover, the results indicate that even very general words like 'good' and 'awesome' set apart this strategy from the other in the data. Hence, the adjectives 'good' and 'awesome' are used much more often in posts of shows that use the linear strategy, which is a result that clearly indicates a linear strategy would be the better option in terms of more positive posts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
